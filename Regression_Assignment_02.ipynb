{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-01    Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression, R-squared is a statistical measure that represents the proportion of the variance in the dependent variable that can be explained by the independent variable(s) in the model. It's essentially a way to quantify how well the regression line fits the actual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`What it represents` :**\n",
    "\n",
    "* Imagine you have a bunch of data points scattered around, and you fit a regression line through them. The total variability in the data can be represented by the sum of the squared distances between each data point and the mean of all the points (this is called the Total Sum of Squares, or SST).\n",
    "\n",
    "* Now, the part of this variability that your regression line explains is the squared distance between each data point and the corresponding point on the regression line (this is called the Residual Sum of Squares, or SSR).\n",
    "\n",
    "* R-squared takes the ratio of these two values, subtracts it from 1, and multiplies by 100% to express it as a percentage. So, it basically tells you what percentage of the total variability in the data is captured by your regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it's calculated:**\n",
    "\n",
    "The formula for R-squared is:\n",
    "\n",
    "\n",
    "$$R^2 = 1 - (SSR/SST) * 100%$$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "* SSR is the Residual Sum of Squares (explained above)\n",
    "* SST is the Total Sum of Squares (explained above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "* A higher R-squared value (closer to 100%) generally indicates a better fit, meaning the regression line explains a larger portion of the variance in the data. However, it's important to keep in mind that:\n",
    "\n",
    "    * A high R-squared doesn't necessarily guarantee a good model. Other factors like model assumptions and potential outliers should also be considered.\n",
    "\n",
    "    * Comparing R-squared values is only meaningful for models fit to the same data and with the same number of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-02    Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusted R-squared and regular R-squared are both measures of how well a statistical model fits a set of data, but they differ in one key aspect: **accounting for the number of predictors in the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regular R-squared:**\n",
    "\n",
    "* Represents the proportion of variance in the dependent variable (what you're trying to predict) that is explained by the independent variables (the predictors).\n",
    "\n",
    "* It ranges from 0 to 1, with 1 indicating a perfect fit and 0 indicating no explanatory power.\n",
    "\n",
    "* However, a higher R-squared doesn't necessarily mean a better model. This is because adding more predictors to the model will almost always increase R-squared, even if the new variables don't provide any meaningful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusted R-squared:**\n",
    "\n",
    "* Takes into account the number of predictors in the model and penalizes the model for overfitting.\n",
    "\n",
    "* It adjusts the R-squared value downward to compensate for the potential inflation due to adding more variables.\n",
    "\n",
    "* It usually ranges from 0 to 1, but can be negative if the model performs worse than simply using the mean as a predictor.\n",
    "\n",
    "* A higher adjusted R-squared generally indicates a better model than a higher regular R-squared, especially when comparing models with different numbers of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key differences:**\n",
    "\n",
    "| Feature | Regular R-squared | Adjusted R-squared |\n",
    "|---|---|---|\n",
    "| Considers number of predictors | No | Yes |\n",
    "| Affected by adding irrelevant variables | Increases | May decrease if variable doesn't add value |\n",
    "| Range | 0 to 1 | 0 to 1 (can be negative) |\n",
    "| Usefulness for comparing models | Limited | Preferred for comparing models with different numbers of predictors |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-03    When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is more appropriate to use Adjusted R-squared in the situations were :\n",
    "\n",
    "*    **we have to Comparing models with different numbers of predictors** \n",
    "\n",
    "Regular R-squared simply increases with the number of predictors added to the model, even if those predictors don't genuinely improve the explanation of the dependent variable. Adjusted R-squared penalizes for the number of predictors, providing a fairer comparison when evaluating models with varying complexity.\n",
    "\n",
    "*    **In Understanding overfitting** \n",
    "\n",
    "When a model has too many predictors relative to the data points, it tends to \"overfit\" the training data, leading to poor performance on unseen data. Adjusted R-squared helps identify this issue by decreasing its value if adding more predictors doesn't genuinely improve the model's fit.\n",
    "\n",
    "*    **We Focusing on model fit as a measure of generalizability** \n",
    "    \n",
    "Adjusted R-squared is generally considered a better indicator of how well a model will perform on new data compared to regular R-squared, making it more relevant for practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-04    What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression analysis, **RMSE (Root Mean Squared Error)**, **MSE (Mean Squared Error)**, and **MAE (Mean Absolute Error)** are commonly used metrics to evaluate the performance of a model. They all measure the difference between the predicted values and the actual values of the target variable, but they do so in slightly different ways :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **MSE (Mean Squared Error)**\n",
    "\n",
    "        * **Calculation -** MSE is calculated by squaring the differences between the predicted and actual values for each data point, then averaging these squared differences across all data points.\n",
    "\n",
    "        * **Interpretation -** MSE represents the average squared error. Higher values indicate larger errors on average, and lower values indicate better model performance. However, interpreting the magnitude of MSE can be difficult because its units are squared units of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **RMSE (Root Mean Squared Error)**\n",
    "\n",
    "        * **Calculation -** RMSE is simply the square root of MSE.\n",
    "\n",
    "        * **Interpretation -** RMSE shares the same interpretation as MSE but has the advantage of being in the same units as the target variable, making it easier to understand its magnitude. Lower RMSE values indicate better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **MAE (Mean Absolute Error)**\n",
    "\n",
    "        * **Calculation -** MAE is calculated by taking the absolute value of the differences between the predicted and actual values for each data point, then averaging these absolute differences across all data points.\n",
    "\n",
    "        * **Interpretation -** MAE represents the average absolute error. Unlike MSE and RMSE, it does not weigh large errors more heavily than small errors. This can be advantageous in cases where you are less concerned about outliers and more interested in the average magnitude of errors. Lower MAE values indicate better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-05    Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Each metric has its own advantages and disadvantages` :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`RMSE`**\n",
    "\n",
    "*    **Advantages -**\n",
    "\n",
    "        * **Interpretability -** Has the same units as the target variable, making interpretation easier.\n",
    "        * **Penalizes larger errors -** Squares large errors, giving them more weight than smaller ones. This can be helpful when outliers are a concern.\n",
    "\n",
    "*    **Disadvantages -**\n",
    "\n",
    "        * **Sensitive to outliers -** Outliers are heavily penalized, potentially skewing the overall result.\n",
    "        * **Not scale-invariant -** Affected by the scale of the target variable, making comparisons across different datasets difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`MSE`**\n",
    "\n",
    "*    **Advantages -**\n",
    "\n",
    "        * **Simple to calculate -** Easier to compute than RMSE, especially for large datasets.\n",
    "        * **Good for gradient-based optimization -** Used in many optimization algorithms due to its differentiable nature.\n",
    "\n",
    "*    **Disadvantages -**\n",
    "\n",
    "        * **Sensitivity to outliers -** Similar to RMSE, outliers have a disproportionate impact.\n",
    "        * **Not interpretable -** Units are squared differences from the target variable, making interpretation less straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`MAE`**\n",
    "\n",
    "*    **Advantages -**\n",
    "\n",
    "        * **Robust to outliers -** Unaffected by large errors since absolute values are used.\n",
    "        * **Interpretable -** Similar to RMSE, has the same units as the target variable.\n",
    "\n",
    "*    **Disadvantages -**\n",
    "\n",
    "        * **Ignores the magnitude of errors -** Doesn't differentiate between large and small underestimations/overestimations.\n",
    "        * **Less sensitive to small errors -** Smaller errors have less impact on the metric compared to larger ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-06    Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Least Absolute Shrinkage and Selection Operator` (LASSO)** is a technique used in statistics and machine learning to improve the performance of models, particularly in linear regression. \n",
    "\n",
    "*    It combines two key functionalities :\n",
    "\n",
    "        1. **Regularization -** This helps prevent **overfitting**, where the model memorizes the training data too closely and fails to generalize to unseen data. Lasso achieves this by adding a penalty term to the objective function that shrinks the magnitude of the coefficients (feature weights) in the model. \n",
    "\n",
    "        2. **Feature Selection -** By shrinking some coefficients to exactly zero, Lasso effectively removes those features from the model. This simplifies the model and improves its interpretability by highlighting the most important features for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Lasso vs. Ridge Regularization`**\n",
    "\n",
    "**Ridge regularization, another popular technique, also penalizes the model complexity but uses a different penalty term based on the squared sum of coefficients.**\n",
    "\n",
    "*    Here's how they differ:\n",
    "\n",
    "        * **Penalty Term -** Lasso uses an L1 norm penalty (sum of absolute values), while Ridge uses an L2 norm penalty (sum of squares).\n",
    "\n",
    "        * **Coefficient Shrinkage -** Lasso shrinks coefficients towards zero, potentially setting some to zero for feature selection. Ridge shrinks coefficients towards zero but keeps all non-zero.\n",
    "\n",
    "        * **Model Sparsity -** Lasso encourages sparse models with fewer non-zero coefficients, leading to better interpretability and potentially overcoming multicollinearity. Ridge generally results in denser models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`When to Use Lasso`**\n",
    "\n",
    "*    Lasso is particularly suitable in several situations:\n",
    "\n",
    "        * **High dimensionality -** When you have many features, Lasso can help select the most important ones, reducing model complexity and potentially improving generalization.\n",
    "\n",
    "        * **Multicollinearity -** If features are highly correlated, Lasso can choose a single representative feature, avoiding issues with multicollinearity.\n",
    "\n",
    "        * **Interpretability -** If understanding the model's behavior is crucial, Lasso's feature selection can reveal which features drive the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-07    How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularized linear models are a powerful tool in machine learning to combat the problem of **overfitting**. Overfitting occurs when a model learns the training data too closely, even capturing random noise, and fails to generalize well to unseen data. This leads to poor performance on new examples.\n",
    "\n",
    "Regularization techniques add a penalty to the model's objective function, which penalizes complex models and encourages simpler ones. This effectively trades off fitting the training data perfectly with keeping the model generalizable. \n",
    "\n",
    "**`There are two main types of regularization` :**\n",
    "\n",
    "1. **L1 Regularization (Lasso) -** This shrinks the coefficients of the model towards zero, essentially removing some features or reducing their influence. This can lead to sparse models with few non-zero coefficients.\n",
    "        \n",
    "2. **L2 Regularization (Ridge) -** This shrinks the coefficients towards zero but not as aggressively as L1. It penalizes the magnitude of the coefficients, keeping all features but reducing their impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Here's an example to illustrate how regularization prevents overfitting` :**\n",
    "\n",
    "*    **Scenario:** Imagine you have a dataset of house prices with features like size, location, and number of bedrooms. You build a linear regression model to predict house prices.\n",
    "\n",
    "*    **Without Regularization:** The model might fit the training data perfectly, capturing even tiny fluctuations in price due to random noise. It might assign large coefficients to irrelevant features like a specific street name. However, this model wouldn't generalize well to new houses.\n",
    "\n",
    "*    **With L1 Regularization:** The penalty term in the objective function pushes some coefficients to zero, effectively removing irrelevant features from the model. This makes the model simpler and less likely to overfit to noise. It might lose some accuracy on the training data but will likely perform better on new houses.\n",
    "\n",
    "*    **With L2 Regularization:** The coefficients are shrunk towards zero but not removed completely. All features remain in the model, but their impact on the prediction is reduced. This can still prevent overfitting while retaining some flexibility compared to L1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.N0-08    Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Regularized linear models`**, while immensely valuable tools, do have limitations that make them unsuitable for certain situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Here are some key points to consider` :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **Oversimplification**\n",
    "\n",
    "        * **Loss of information -** Regularization shrinks model coefficients toward zero, which can remove important information from features, leading to underfitting and reduced predictive power.\n",
    "\n",
    "        * **Poor handling of complex relationships -** If the true relationship between features and the target variable is non-linear or involves complex interactions, regularization might oversimplify it, reducing model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **Assumptions and limitations**\n",
    "\n",
    "        * **Equal feature importance -** Most regularization techniques treat all features equally, whereas in reality, some features might be more influential. This can lead to inaccurate interpretations of feature importance.\n",
    "        \n",
    "        * **Limited applicability to non-linear problems -** While regularized linear models can capture some non-linearity through feature engineering, they are inherently linear and struggle with truly non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **Tuning challenges:**\n",
    "\n",
    "        * **Finding the optimal hyperparameter -** Choosing the right regularization strength (e.g., lambda in Ridge regression) heavily impacts performance. Finding the optimal value requires careful tuning, which can be computationally expensive and subjective.\n",
    "        \n",
    "        * **Sensitivity to outliers -** Regularization can be sensitive to outliers, as they can unduly influence the estimated coefficients. Careful data preprocessing is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **Interpretability issues**\n",
    "\n",
    "        * **Black box nature -** Certain regularization techniques (LASSO) introduce sparsity (setting some coefficients to zero), making it harder to interpret the model and understand how features contribute to the prediction.\n",
    "        \n",
    "        * **Multicollinearity -** Regularization can mask multicollinearity issues, where features are highly correlated. While it addresses the statistical problem, it hinders feature interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **Alternatives to consider**\n",
    "\n",
    "        *    When these limitations outweigh the benefits, other regression methods might be more suitable:\n",
    "\n",
    "                * **Polynomial regression or splines -** For capturing non-linear relationships.\n",
    "        \n",
    "                * **Support Vector Regression (SVR) -** Robust to outliers and handles non-linearity to some extent.\n",
    "        \n",
    "                * **Tree-based methods (e.g., Random Forests) -** Less sensitive to feature scaling and can handle complex interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **Conclusion**\n",
    "\n",
    "        *    Regularized linear models are powerful tools, but they are not a one-size-fits-all solution. Understanding their limitations, assumptions, and tuning complexities is crucial to determine when they are the best choice for your regression analysis task. Consider the data characteristics, problem complexity, and desired interpretations when making your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No-09    You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the given information, it is not possible to definitively say which model, A or B, is the better performer just by comparing their RMSE and MAE values. The choice of a better model depends on the specific use case, the data distribution, and the business context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`RMSE` (Root Mean Squared Error)** is more sensitive to outliers due to the squaring operation, which amplifies the effect of large errors. \n",
    "\n",
    "**`MAE` (Mean Absolute Error)** is less sensitive to outliers, as it takes the absolute difference between predicted and actual values, without squaring them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*    **If the use case is not sensitive to outliers, Model B with an MAE of 8 might be a better choice, as it has a lower error on average.** \n",
    "\n",
    "*    **If the use case is sensitive to outliers, Model A with an RMSE of 10 might be a better choice, as it might handle outliers more gracefully.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are limitations to using both `RMSE` and `MAE` as evaluation metrics.** \n",
    "\n",
    "RMSE might be misleading if the data contains outliers, as it amplifies the effect of large errors. MAE, on the other hand, might be less informative, as it does not differentiate between small and large errors. In some cases, it might be beneficial to consider other evaluation metrics, such as Mean Absolute Percentage Error (MAPE), Mean Bias Error (MBE), or others, depending on the specific use case and data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q.No.10    You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`To compare the performance of Modle A and Model B`**, \n",
    "\n",
    "*   Model A\n",
    "\n",
    "     *    Ridge regularization with alpha = 0.1\n",
    "\n",
    "*   Model B\n",
    "\n",
    "     *    Lasso regularization with = 0.5\n",
    "\n",
    "\n",
    "**we need to the impact of these regularization parameters on the models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Ridge regression` (Model A)** uses L2 regularization, which adds a penalty equal to the square of the magnitude of the coefficients to the loss function. This tends to shrink the coefficients towards zero but not set them exactly to zero. Ridge regression is more suitable when we have a lot of correlated predictors, as it keeps all the variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Lasso regression` (Model B)** uses L1 regularization, which adds a penalty equal to the absolute value of the magnitude of the coefficients to the loss function. This tends to shrink some coefficients to zero, effectively excluding those variables from the model. Lasso regression is more suitable when we have a lot of irrelevant or redundant predictors, as it can help in feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The choice between Model A and Model B depends on the specific problem and the nature of the data.` \n",
    "\n",
    "*    **If we have a lot of correlated predictors, Model A might be a better choice.**\n",
    "\n",
    "*    **If we have a lot of irrelevant or redundant predictors, Model B might be a better choice.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, `there are trade-offs and limitations to both methods`. \n",
    "\n",
    "*    **Ridge regression might not be as effective in reducing overfitting as Lasso regression, especially when there are irrelevant or redundant predictors.** \n",
    "\n",
    "*    **Lasso regression might be too aggressive in feature selection, potentially excluding important predictors from the model.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
